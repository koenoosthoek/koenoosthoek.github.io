---
title: "How do the words in headlines develop in the week before Christmas 2021?"
subtitle: "The final data science project for the course [Introduction to Data Science](https://github.com/intro-to-data-science-21) taught by Simon Munzert at the Hertie School, Berlin, in Fall 2021."
author: 
- Abhiram Mohan | [GitHub](https://github.com/Abhilearns2code) | a.mohan@mpp.hertie-school.org | 205928
- Koen Oosthoek | [GitHub](https://github.com/koenoosthoek) | k.oosthoek@mpp.hertie-school.org | 204638
output: 
  html_document:
    toc: FALSE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: "cosmo"
    toc_depth: 3
    toc_float: false
    css: custom.css 
    self_contained: false
---

<style type="text/css">
  body{
  font-size: 13pt;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) 
```

```{r message = FALSE, warning = FALSE, include = FALSE}
## R Markdown 

library(tidyverse)
library(fs)
library(tidytext)
library(fs)
library(plotly)
library(kableExtra)
```

```{r message = FALSE, warning = FALSE, include = FALSE}
# importing csv files -----------------------------------------------------
file_paths <- fs::dir_ls("data")
file_paths

file_contents <- list()

for (i in seq_along(file_paths)) {
  file_contents[[i]] <- read_csv(
    file = file_paths[[i]]
  )
}

file_contents <- set_names(file_contents, file_paths)

headlines <- do.call(rbind, file_contents)
```

```{r message = FALSE, warning = FALSE, include = FALSE}
# cleaning with function - before tokenizing! --------------------------------------------------

clean_data <- function(x) { # x = path
  df <- x
  df$time <- as.POSIXct(df$time) %>% as.character() %>% str_replace_all("[ :]", "-")
  df <- select(df, -"...1")
  df$value <- gsub("<.*?>", "", df$value) # remove html tages
  df$value <- gsub(' +',' ', df$value) # remove more than one space
  df$value <- str_trim(df$value, side = c("both", "left", "right")) # remove spaces at start and end
  df[df == ""] <- NA #adding NAs to blank cells
  df <- na.omit(df) # remove NAs
  df <- rename(df, headline = value)
}

headlines <- clean_data(headlines)
```

```{r message = FALSE, warning = FALSE, include = FALSE}
headlines <- headlines %>% # maximum of 5500 headlines to avoid distortion, escecially of uk newspaper dailymail
  mutate(country = dplyr::case_when(name == "guardian" ~ "UK",
                                    name == "times" ~ "UK",
                                    name == "dailymail" ~ "UK",
                                    name == "independent" ~ "UK",
                                    name == "mirror" ~ "UK",
                                    name == "telegraph" ~ "UK",
                                    name == "nytimes" ~ "US",
                                    name == "wsj" ~ "US",
                                    name == "usatoday" ~ "US",
                                    name == "washingtonpost" ~ "US",
                                    name == "latimes" ~ "US",
                                    name == "tampabay" ~ "US",
                                    name == "heraldsun" ~ "Australia",
                                    name == "dailytelegraph" ~ "Australia",
                                    name == "financialreview" ~ "Australia",
                                    name == "couriermail" ~ "Australia",
                                    name == "westaustralian" ~ "Australia",
                                    name == "advertiser" ~ "Australia",
                                    name == "globeandmail" ~ "Canada",
                                    name == "thestar" ~ "Canada",
                                    name == "nationalpost" ~ "Canada",
                                    name == "torontosun" ~ "Canada",
                                    name == "vancouversun" ~ "Canada",
                                    name == "montrealgazette" ~ "Canada",
                                    name == "nzherald" ~ "New-Zealand",
                                    name == "waikato" ~ "New-Zealand",
                                    name == "businessreview" ~ "New-Zealand",
                                    name == "gisborneherald" ~ "New-Zealand",
                                    name == "dominionpost" ~ "New-Zealand",
                                    name == "thepress" ~ "New-Zealand"),
         
          format = dplyr::case_when(name == "dailymail" ~ "tabloid",
                                    name == "mirror" ~ "tabloid",
                                    name == "couriermail" ~ "tabloid",
                                    name == "westaustralian" ~ "tabloid",
                                    name == "advertiser" ~ "tabloid",
                                    name == "torontosun" ~ "tabloid",
                                    name == "nzherald" ~ "tabloid",
                                    TRUE ~ "broadsheet"),
         
         day = dplyr::case_when(grepl("2021-12-09", time) ~ "December 9",
                                grepl("2021-12-10", time) ~ "December 10",
                                grepl("2021-12-11", time) ~ "December 11",
                                grepl("2021-12-12", time) ~ "December 12",
                                grepl("2021-12-13", time) ~ "December 13",
                                grepl("2021-12-14", time) ~ "December 14",
                                grepl("2021-12-15", time) ~ "December 15",
                                grepl("2021-12-16", time) ~ "December 16"))

```

```{r message = FALSE, warning = FALSE, include = FALSE}
# ensuring balance --------------------------------------------------------
headlines <- headlines %>% 
  group_by(name) %>% 
  slice(1:5000) %>% 
  ungroup()

# tokenizing --------------------------------------------------------------
headlines_tok <- headlines %>% 
  unnest_tokens(output = word, input = headline) %>% 
  anti_join(stop_words) 
# clean -------------------------------------------------------------------

headlines_tok <- headlines_tok %>% 
  filter(!grepl('content|subscriber|read|star|10|9|6|min|l.a|wa|nz|canada|uk|west|tampa|times|2021|2022|home|life
                |time|top|city|day|people|world|house|win|reveals|revealed|dont|1|amid|live', word))  # daily mail uses the word star everywhore
```

```{r message = FALSE, warning = FALSE, include = FALSE}
# ensuring balance -------------------------------------------------------

Number <-1



headlines_tok["Number"] <- Number



Alldays<-headlines_tok %>%
        group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 




Alldays$word <- factor(Alldays$word, levels = Alldays$word[order(Alldays$occurrences)])

 Alldays <- Alldays %>% ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words across the world ")) + 
     geom_blank()





## December 9 all countries ##


December9_All<- headlines_tok %>%
filter(day == "December 9") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 

December9_All$word <- factor(December9_All$word, levels = December9_All$word[order(December9_All$occurrences)])

    December9_All<-December9_All %>%  ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by all the newspapers on December 9")) +
     geom_blank()
    
   
  

 December9_UK <- headlines_tok %>%
filter(day == "December 9"&
country == "UK") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December9_UK$word <- factor(December9_UK$word, levels = December9_UK$word[order(December9_UK$occurrences)])
 
 
 December9_UK <- December9_UK %>%  ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +                                  
     ggtitle(paste("The 20 most frequently used words by UK newspapers on December 9th ")) + 
     geom_blank()
 
 

 
 
 December9_Us <- headlines_tok %>%
filter(day == "December 9"&
country == "US") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 December9_Us$word <- factor(December9_Us$word, levels = December9_Us$word[order(December9_Us$occurrences)])
 
 
 December9_Us <- December9_Us %>%  ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by US newspapers on December 9")) +
     geom_blank()
 
 
 December9_Canada <-headlines_tok %>%
filter(day == "December 9"&
country == "Canada") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December9_Canada$word <- factor(December9_Canada$word, levels = December9_Canada$word[order(December9_Canada$occurrences)])
 
 
 December9_Canada <- December9_Canada %>% ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Canada newspapers on December 9")) +
     geom_blank()
 
 December9_NZ <-headlines_tok %>%
filter(day == "December 9"&
country == "New-Zealand") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 December9_NZ$word <- factor(December9_NZ$word, levels = December9_NZ$word[order(December9_NZ$occurrences)])
 
 
 December9_NZ <- December9_NZ %>% ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by New Zealand newspapers on December 9")) +
     geom_blank()
 
 
 December9_Aus <-headlines_tok %>%
filter(day == "December 9"&
country == "Australia") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December9_Aus$word <- factor(December9_Aus$word, levels = December9_Aus$word[order(December9_Aus$occurrences)])
 
 
 December9_Aus <- December9_Aus %>% ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Australian newspapers on December 9")) +
     geom_blank()

 
 ## December 10 all countries ##
 
 
 December10_All<- headlines_tok %>%
filter(day == "December 10") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 December10_All$word <- factor(December10_All$word, levels = December10_All$word[order(December10_All$occurrences)])
 
 December10_All <-  December10_All %>%  ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by all the newspapers on December 10")) +
     geom_blank()
 
December10_UK<-headlines_tok %>%
filter(day == "December 10"&
country == "UK") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()  
 
December10_UK$word <- factor(December10_UK$word, levels = December10_UK$word[order(December10_UK$occurrences)])
 
 December10_UK <-  December10_UK %>%
ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by UK newspapers on December 10")) +
     geom_blank()
 
 
December10_US <- headlines_tok %>%
filter(day == "December 10"&
country == "US") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 

December10_US$word <- factor(December10_US$word, levels = December10_US$word[order(December10_US$occurrences)])

December10_US <- December10_US %>% 
    ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by US newspapers on December 10")) +
     geom_blank()
 
 
 December10_Canada <-headlines_tok %>%
filter(day == "December 10"&
country == "Canada") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 December10_Canada$word <- factor(December10_Canada$word, levels = December10_Canada$word[order(December10_Canada$occurrences)])

 
  December10_Canada <- December10_Canada %>%   ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Canada newspapers on December 10")) +
     geom_blank()
 
December10_NZ <- headlines_tok %>%
filter(day == "December 10"&
country == "New-Zealand") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 

December10_NZ$word <- factor(December10_NZ$word, levels = December10_NZ$word[order(December10_NZ$occurrences)])


 December10_NZ <- December10_NZ %>%    ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by New Zealand newspapers on December 10")) +
     geom_blank()
 
 
 December10_Aus <- headlines_tok %>%
filter(day == "December 10"&
country == "Australia") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 December10_Aus$word <- factor(December10_Aus$word, levels = December10_Aus$word[order(December10_Aus$occurrences)])
 
 
 December10_Aus <-December10_Aus %>%   ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Australia newspapers on December 10")) +
     geom_blank()
 
 ## December 11 all countries ##
 
 
 
 December11_All<- headlines_tok %>%
filter(day == "December 11") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December11_All$word <- factor(December11_All$word, levels = December11_All$word[order(December11_All$occurrences)])
 
   December11_All <-  December11_All %>%  ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by all the newspapers on December 11")) +
     geom_blank()
   
 
 December11_UK<-headlines_tok %>%
filter(day == "December 11"&
country == "UK") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 December11_UK$word <- factor(December11_UK$word, levels = December11_UK$word[order(December11_UK$occurrences)])
 
 December11_UK <- December11_UK %>%   ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by UK newspapers on December 11")) +
     geom_blank()
 
 
 
 
 December11_US<- headlines_tok %>%
filter(day == "December 11"&
country == "US") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 

 December11_US$word <- factor(December11_US$word, levels = December11_US$word[order(December11_US$occurrences)]) 
 
 
December11_US <-  December11_US %>%   ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by US newspapers on December 11")) +
     geom_blank()
 
 
 December11_Canada <-headlines_tok %>%
filter(day == "December 11"&
country == "Canada") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December11_Canada$word <- factor(December11_Canada$word, levels = December11_Canada$word[order(December11_Canada$occurrences)]) 
 
 December11_Canada <-December11_Canada %>%  ggplot(aes(x=word, y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Canada newspapers on December 11")) +
     geom_blank()
 
 December11_NZ <-headlines_tok %>%
filter(day == "December 11"&
country == "New-Zealand") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 December11_NZ$word <- factor(December11_NZ$word, levels = December11_NZ$word[order(December11_NZ$occurrences)]) 
 
  December11_NZ <-December11_NZ %>%   ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by New Zealand newspapers on December 11")) +
     geom_blank()
 
 
 December11_Aus <-headlines_tok %>%
filter(day == "December 11"&
country == "Australia") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 December11_Aus$word <- factor(December11_Aus$word, levels = December11_Aus$word[order(December11_Aus$occurrences)]) 
 
 
 
   December11_Aus <- December11_Aus %>% ggplot(aes(x= word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Australia newspapers on December 11")) +
     geom_blank()
 
 ## December 12 all countries ##
 
 
 December12_All<- headlines_tok %>%
filter(day == "December 12") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December12_All$word <- factor(December12_All$word, levels = December12_All$word[order(December12_All$occurrences)])
 
 
 
December12_All <-December12_All %>% ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by all newspapers on December 12")) +
     geom_blank()
 
 December12_UK <- headlines_tok %>%
filter(day == "December 12"&
country == "UK") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December12_UK$word <- factor(December12_UK$word, levels = December12_UK$word[order(December12_UK$occurrences)])
 
 
 
December12_UK <-December12_UK %>% 
 
    ggplot(aes(x=word,
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by UK newspapers on December 12")) +
     geom_blank()
 
 
 
 
 December12_US <-headlines_tok %>%
filter(day == "December 12"&
country == "US") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December12_US$word <- factor(December12_US$word, levels = December12_US$word[order(December12_US$occurrences)])
 
 
 
December12_US <-December12_US %>% 
    ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by US newspapers on December 12")) +
     geom_blank()
 
 
 December12_Canada <-headlines_tok %>%
filter(day == "December 12"&
country == "Canada") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 December12_Canada$word <- factor(December12_Canada$word, levels = December12_Canada$word[order(December12_Canada$occurrences)])
 
 
 
December12_Canada <-December12_Canada %>%
    ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Canada newspapers on December 12")) +
     geom_blank()



 
 December12_NZ <-headlines_tok %>%
filter(day == "December 12"&
country == "New-Zealand") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December12_NZ$word <- factor(December12_NZ$word, levels = December12_NZ$word[order(December12_NZ$occurrences)])
 
 
  December12_NZ <-December12_NZ %>%  ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by New Zealand newspapers on December 12")) +
     geom_blank()
 
 
 December12_Aus <-headlines_tok %>%
filter(day == "December 12"&
country == "Australia") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 December12_Aus$word <- factor(December12_Aus$word, levels = December12_Aus$word[order(December12_Aus$occurrences)])
 
 
 December12_Aus <-December12_Aus %>% 
   ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Australia newspapers on December 9")) +
     geom_blank()
 
 ## December 13 ##
 
 December13_All<- headlines_tok %>%
filter(day == "December 13") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 
 
 December13_All$word <- factor(December13_All$word, levels = December13_All$word[order(December13_All$occurrences)])
 
    December13_All <-December13_All %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by all newspapers on December 13")) +
     geom_blank()
 
 
 
December13_UK <-headlines_tok %>%
filter(day == "December 13"&
country == "UK") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 


    December13_UK$word <- factor(December13_UK$word, levels = December13_UK$word[order(December13_UK$occurrences)])
 
    December13_UK <-December13_UK %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by UK newspapers on December 13")) +
     geom_blank()
 
 
 
 
 December13_US <-headlines_tok %>%
filter(day == "December 13"&
country == "US") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 
     December13_US$word <- factor(December13_US$word, levels = December13_US$word[order(December13_US$occurrences)])
 
    December13_US <-December13_US %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by US newspapers on December 13")) +
     geom_blank()
 
 
    
    
    
  
 December13_Canada <-headlines_tok %>%
filter(day == "December 13"&
country == "Canada") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December13_Canada$word <- factor(December13_Canada$word, levels = December13_Canada$word[order(December13_Canada$occurrences)])
 
    December13_Canada <-December13_Canada %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Canada newspapers on December 13")) +
     geom_blank()
    
    
  
    
 
 December13_NZ <-headlines_tok %>%
filter(day == "December 13"&
country == "New-Zealand") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 
 
    December13_NZ$word <- factor(December13_NZ$word, levels = December13_NZ$word[order(December13_NZ$occurrences)])
 
    December13_NZ <-December13_NZ %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by New Zealand newspapers on December 13")) +
     geom_blank()
 
    
 
 December13_Aus <-headlines_tok %>%
filter(day == "December 13"&
country == "Australia") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 December13_Aus$word <- factor(December13_Aus$word, levels = December13_Aus$word[order(December13_Aus$occurrences)])
 
    December13_Aus <-December13_Aus %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Australia newspapers on December 13")) +
     geom_blank()
 
 
 ## December 14##
 
 December14_All<- headlines_tok %>%
filter(day == "December 14") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 December14_All$word <- factor(December14_All$word, levels = December14_All$word[order(December14_All$occurrences)])
 
    December14_All <-December14_All %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by all newspapers on December 14")) +
     geom_blank()
    
    
 
 December14_UK <-headlines_tok %>%
filter(day == "December 14"&
country == "UK") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
    December14_UK$word <- factor(December14_UK$word, levels = December14_UK$word[order(December14_UK$occurrences)])
 
    December14_UK <-December14_UK %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by UK newspapers on December 14")) +
     geom_blank()
 
 
 
 
 December14_US <-headlines_tok %>%
filter(day == "December 14"&
country == "US") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
  
 
 
   December14_US$word <- factor(December14_US$word, levels = December14_US$word[order(December14_US$occurrences)])
 
    December14_US <-December14_US %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by US newspapers on December 14")) +
     geom_blank()
    
    
 
 
 December14_Canada <-headlines_tok %>%
filter(day == "December 14"&
country == "Canada") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
    December14_Canada$word <- factor(December14_Canada$word, levels = December14_Canada$word[order(December14_Canada$occurrences)])
 
    December14_Canada <-December14_Canada %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Canada newspapers on December 14")) +
     geom_blank()
 
    
    
    
 December14_NZ <-headlines_tok %>%
filter(day == "December 14"&
country == "New-Zealand") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
December14_NZ$word <- factor(December14_NZ$word, levels = December14_NZ$word[order(December14_NZ$occurrences)])
 
    December14_NZ <-December14_NZ %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by New Zealand newspapers on December 14")) +
     geom_blank()
 
 
 December14_Aus <-headlines_tok %>%
filter(day == "December 14"&
country == "Australia") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 
   December14_Aus$word <- factor(December14_Aus$word, levels = December14_Aus$word[order(December14_Aus$occurrences)])
 
    December14_Aus <-December14_Aus %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Australia newspapers on December 14")) +
     geom_blank()
 
 
 ## December 15
 
 December15_All<- headlines_tok %>%
filter(day == "December 15") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
 
    December15_All$word <- factor(December15_All$word, levels = December15_All$word[order(December15_All$occurrences)])
 
    December15_All <-December15_All %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by all newspapers on December 15")) +
     geom_blank()
 
 
 
 
December15_UK <- headlines_tok %>%
filter(day == "December 15"&
country == "UK") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 



   December15_UK$word <- factor(December15_UK$word, levels = December15_UK$word[order(December15_UK$occurrences)])
 
    December15_UK <-December15_UK %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by UK newspapers on December 15")) +
     geom_blank()
 
 
 
 
 December15_US <-headlines_tok %>%
filter(day == "December 15"&
country == "US") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup()
 
 
    December15_US$word <- factor(December15_US$word, levels = December15_US$word[order(December15_US$occurrences)])
 
    December15_US <-December15_US %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by US newspapers on December 15")) +
     geom_blank()
 
 
 December15_Canada <-headlines_tok %>%
filter(day == "December 15"&
country == "Canada") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
    December15_Canada$word <- factor(December15_Canada$word, levels = December15_Canada$word[order(December15_Canada$occurrences)])
 
    December15_Canada <-December15_Canada %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Canada newspapers on December 15")) +
     geom_blank()
 
 December15_NZ <- headlines_tok %>%
filter(day == "December 15"&
country == "New-Zealand") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
    December15_NZ$word <- factor(December15_NZ$word, levels = December15_NZ$word[order(December15_NZ$occurrences)])
 
    December15_NZ <-December15_NZ %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by New Zealand newspapers on December 15")) +
     geom_blank()
 
 
 December15_Aus <- headlines_tok %>%
filter(day == "December 15"&
country == "Australia") %>%
group_by(word) %>% 
       summarise(occurrences=sum(Number)) %>% 
     arrange(desc(occurrences)) %>% 
      top_n(20) %>%
      ungroup() 
 
 
   December15_Aus$word <- factor(December15_Aus$word, levels = December15_Aus$word[order(December15_Aus$occurrences)])
 
    December15_Aus <-December15_Aus %>%ggplot(aes(x=word, 
               y=occurrences)) +
    geom_col() + xlab("")+
    ylab("Frequency") +
    coord_flip() +
     theme_minimal() +
   scale_fill_gradient(high = "#f6a97a", low="#ca3c97") +
     ggtitle(paste("The 20 most frequently used words by Australia newspapers on December 15")) +
     geom_blank()
 
 
                                               


```

##

---

#### Executive Summary

The modern 24-7 news cycle moves faster ever than before - and is often described as sensational and speculative. We want to assess the claim whether topics are constantly changing and replaced by newer topics. Therefore, we attempt to find out how long certain words in headlines remain in the news between December 9 and 15 2021. For this purpose, we scraped the headlines of six newspapers in five English-speaking countries: the US, UK, Canada, Australia and New-Zealand. Unsurprisingly, our findings show that Covid, Christmas and Omikron are discussed the most in the headlines. Yet we also find that many words that were dominating the headlines on December 9 lose their significance relatively quickly in the days that followed. While this pattern is mostly present in the UK, New-Zealand and Australia, the words in the headlines of the US and Canada like to be discussed for a longer period of time. 

---

#### Introduction

It is often said that “breaking news”, speculative and sensational headlines characterize the modern 24-hour news cycle. In order to survive today’s fast-paced media landscape, media outlets need to compete for audiences that subscribe to their platform. Some authors underline the dangers this may pose to trustworthy information and an an informed public debate more generally - and refer to the “menace of media speed and the 24-hour news cycle”. ^[Rosenberg, H., & Feldman, C. S. (2008). No Time To Think: The mace of media speed and the 24-hour news cycle. Bloomsbury Academic.] To survive in the market, news platforms need to compete and attract as many users as possible. The gain is twofold: if more users subscribe to a platform, this does not just raise revenue in itself, but also makes the platform more attractive to advertisers. 

On the basis of this mechanism, we would expect that in order to stay attractive to their audiences, newspapers adapt their headlines to the latest developments - and change their front page headlines as soon and as much possible. We aim to research if the data provides support for this expectation - by asking the following question: How do the key words in news headlines develop over time? Through our analysis, we aim to gain more insight the question whether the headlines of the news-cycle are constantly changing - adapting to the latest developments - or like to stay in the news-cycle for a longer period of time.

#### Data and methodology

We chose to focus on the words that are used in the front page headlines of the largest newspapers in the US, UK, Canada, Australia and New-Zealand. We scraped over 132.000 front page headlines between December 9 00:01 CET and December 16 20:00 CET, with a time-interval of four hours. We scraped the headlines on the front end of the news websites (with xpaths that link the elements of an HTML page) - and automized this process using a schedueler called the `cronR` package in R. After importing and cleaning the raw data, we truncated the headlines of a few newspapers to ensure balance. We also only wanted to include the words that relate to a genuine topic (such as Christmas or Covid). Therefore, we removed the (stop-) words that have no inherent meaning, such as "click", "home" and "the". Finally, we tokenized the headlines, resulting in a final dataset of 595,000 words. 

*Figure 1: The number of words that were scraped off the front pages of newspaper websites*

## {.tabset .tabset-fade .tabset-pills}

### US {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
us <- headlines_tok %>% 
  filter(country == "US") %>% 
  group_by(name) %>% 
  count() %>% 
  arrange(desc(n))

Us_New <- c("Washington Post","Los Angeles Times","Wallstreet Journal","New York Times","Tampa Bay Times","USA Today")

us <- us %>%
  add_column(Us_New,.before = "name")

us = subset(us, select = -c(name))


us <-us%>% 
  kable(col.names = c("Newspapers","Number of words")) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE,
                position = "center")
```

```{r}
us
```

### UK {.tabset .tabset-fade .tabset-pills}
```{r message = FALSE, warning = FALSE, include = FALSE}
uk <- headlines_tok %>% 
  filter(country == "UK") %>% 
  group_by(name) %>% 
  count() %>% 
  arrange(desc(n)) 

UK_New <-c("Daily Mail","Mirror","The Telegraph","The Independent","The Times","The Guardian")

uk <- uk %>%
  add_column(UK_New,.before = "name")

uk = subset(uk, select = -c(name))

uk <- uk %>%  
  kable(col.names = c("Newspapers","Number of words")) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE,
                position = "center")
```

```{r}
uk
```


### Canada {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
canada <- headlines_tok %>% 
  filter(country == "Canada") %>% 
  group_by(name) %>% 
  count() %>% 
  arrange(desc(n))

Canada_new <-c("The Globe and Mail","Toronto Star","Vancouver Sun"," Montreal Gazette","Toronto Sun"," National Post")

canada <- canada %>%
  add_column(Canada_new,.before = "name")

canada = subset (canada, select = -c(name))

canada <- canada %>% kable(col.names = c("Newspapers","Number of words")) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE,
                position = "center")
```

```{r}
canada
```

### Australia {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}

aus <- headlines_tok %>% 
  filter(country == "Australia") %>% 
  group_by(name) %>% 
  count() %>% 
  arrange(desc(n))

aus_new <-c("West Australian","Courier Mail","The Daily Telegraph","The Advertiser","The Herald Sun"," Financial Review")

aus <- aus %>%
  add_column(aus_new,.before = "name")

aus = subset (aus, select = -c(name))


aus <- aus %>% kable(col.names = c("Newspapers","Number of words")) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE,
                position = "center")
```

```{r}
aus
```


### New-Zealand {.tabset .tabset-fade .tabset-pills}
```{r message = FALSE, warning = FALSE, include = FALSE}
nz <- headlines_tok %>% 
  filter(country == "New-Zealand") %>% 
  group_by(name) %>% 
  count() %>% 
  arrange(desc(n)) 


nz_new <-c("The New-Zealand Herald","Business Review","Gisborne Herald","Waikato","Dominion Post","The Press")

nz <- nz %>%
  add_column(nz_new,.before = "name")

nz = subset (nz, select = -c(name))

nz <- nz %>%  
  kable(col.names = c("Newspapers","Number of words")) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE,
                position = "center")
```

```{r}
nz
```








## 
Figure 1 provides an overview of the proportions of words that we scraped - relative to each newspaper. Newspapers such as the Dailymail (N = 61856), Mirror (N = 39010) and Telegraph (N = 29548) in the UK, the Globe and Mail in Canada (N = 32523) and the New-Zealand Herald (N = 25838) have the highest share of words in our dataset. This is either due to the fact that some of these newspapers have very long digital front pages or because some newspapers like to use very long headlines (e.g the headlines of the UK Dailymail often consist of four to six sentences). Although we tried to balance our data by truncating the words of these newspapers, they still have a relatively high share of words in our data.


*Figure 2: The usage of words for all day and per days*

## {.tabset .tabset-fade .tabset-pills}

### All days {.tabset .tabset-fade .tabset-pills}

```{r, out.width='100%'}
ggplotly(Alldays) %>% 
  partial_bundle()
```

### December 9 {.tabset .tabset-fade .tabset-pills}

#### All countries

```{r, out.width='100%'}
ggplotly(December9_All) %>% 
  partial_bundle()
```

#### US

```{r, out.width='100%'}

ggplotly(December9_Us) %>% 
  partial_bundle()

```

#### UK

```{r, out.width='100%'}

ggplotly(December9_UK) %>% 
  partial_bundle()

```

#### Canada 

```{r, out.width='100%'}
ggplotly(December9_Canada) %>% 
  partial_bundle()
```

#### Australia

```{r, out.width='100%'}
ggplotly(December9_Aus) %>% 
  partial_bundle()
```

#### New-Zealand

```{r, out.width='100%'}

ggplotly(December9_NZ) %>% 
  partial_bundle()

```


### December 10 {.tabset .tabset-fade .tabset-pills}

#### All countries

```{r, out.width='100%'}
ggplotly(December10_All) %>% 
  partial_bundle()
```

#### US

```{r, out.width='100%'}

ggplotly(December10_US) %>% 
  partial_bundle()

```

#### UK

```{r, out.width='100%'}

ggplotly(December10_UK) %>% 
  partial_bundle()

```

#### Canada

```{r, out.width='100%'}

ggplotly(December10_Canada) %>% 
  partial_bundle()

```

#### Australia

```{r, out.width='100%'}

ggplotly(December10_Aus) %>% 
  partial_bundle()

```


#### New-Zealand

```{r, out.width='100%'}

ggplotly(December10_NZ) %>% 
  partial_bundle()

```

### December 11 {.tabset .tabset-fade .tabset-pills}

#### All countries

```{r, out.width='100%'}
ggplotly(December11_All) %>% 
  partial_bundle()
```

#### US

```{r, out.width='100%'}

ggplotly(December11_US) %>% 
  partial_bundle()

```

#### UK

```{r, out.width='100%'}

ggplotly(December11_UK) %>% 
  partial_bundle()

```

#### Canada

```{r, out.width='100%'}

ggplotly(December11_Canada) %>% 
  partial_bundle()

```

#### Australia

```{r, out.width='100%'}

ggplotly(December11_Aus) %>% 
  partial_bundle()

```


#### New-Zealand

```{r, out.width='100%'}

ggplotly(December11_NZ) %>% 
  partial_bundle()

```


### December 12 {.tabset .tabset-fade .tabset-pills}

#### All countries

```{r, out.width='100%'}
ggplotly(December12_All) %>% 
  partial_bundle()
```

#### US

```{r, out.width='100%'}

ggplotly(December12_US) %>% 
  partial_bundle()

```

#### UK

```{r, out.width='100%'}

ggplotly(December12_UK) %>% 
  partial_bundle()

```

#### Canada

```{r, out.width='100%'}

ggplotly(December12_Canada) %>% 
  partial_bundle()

```

#### Australia

```{r, out.width='100%'}

ggplotly(December12_Aus) %>% 
  partial_bundle()

```


#### New-Zealand

```{r, out.width='100%'}

ggplotly(December12_NZ) %>% 
  partial_bundle()

```

### December 13 {.tabset .tabset-fade .tabset-pills}

#### All countries

```{r, out.width='100%'}
ggplotly(December13_All) %>% 
  partial_bundle()
```

#### US

```{r, out.width='100%'}

ggplotly(December13_US) %>% 
  partial_bundle()

```

#### UK

```{r, out.width='100%'}

ggplotly(December13_UK) %>% 
  partial_bundle()

```

#### Canada

```{r, out.width='100%'}

ggplotly(December13_Canada) %>% 
  partial_bundle()

```

#### Australia

```{r, out.width='100%'}

ggplotly(December13_Aus) %>% 
  partial_bundle()

```


#### New-Zealand

```{r, out.width='100%'}

ggplotly(December13_NZ) %>% 
  partial_bundle()

```

### December 14 {.tabset .tabset-fade .tabset-pills}

#### All countries

```{r, out.width='100%'}
ggplotly(December14_All) %>% 
  partial_bundle()
```

#### US

```{r, out.width='100%'}

ggplotly(December14_US) %>% 
  partial_bundle()

```

#### UK

```{r, out.width='100%'}

ggplotly(December14_UK) %>% 
  partial_bundle()

```

#### Canada

```{r, out.width='100%'}

ggplotly(December14_Canada) %>% 
  partial_bundle()

```

#### Australia

```{r, out.width='100%'}

ggplotly(December14_Aus) %>% 
  partial_bundle()

```


#### New-Zealand

```{r, out.width='100%'}

ggplotly(December14_NZ) %>% 
  partial_bundle()

```

### December 15 {.tabset .tabset-fade .tabset-pills}

#### All countries

```{r, out.width='100%'}
ggplotly(December15_All) %>% 
  partial_bundle()
```

#### US

```{r, out.width='100%'}

ggplotly(December15_US) %>% 
  partial_bundle()

```

#### UK

```{r, out.width='100%'}

ggplotly(December15_UK) %>% 
  partial_bundle()

```

#### Canada

```{r, out.width='100%'}

ggplotly(December15_Canada) %>% 
  partial_bundle()

```

#### Australia

```{r, out.width='100%'}

ggplotly(December15_Aus) %>% 
  partial_bundle()

```


#### New-Zealand

```{r, out.width='100%'}

ggplotly(December15_NZ) %>% 
  partial_bundle()

```









##
Turning to *figure 2*, we can observe the twenty words that were most used in headlines. Our results show one consistent and self-explanatory finding: "Covid" and "Christmas" are the most prominent words in the headlines between December 9 and 15. From December 11 onwards, the word "omicron" joins the top-3 and moves to number 2 on December 14. Other health-related words such as "vaccine" and "booster" (from December 13) are also highly prevalent. Moreover, words relating to justice and law enforcement are often mentioned. "Police", "court", "rules", "dead", and "killed" are used relatively frequently on all days. 

On the country level, we identify several interesting observations. In the US, President Biden is given a lot of attention, as well as other topics such as abortion, family, China and "black". From December 12 onwards, the tornado that hit the midwest of the US is the second-most discussed topic in the headlines. In the UK, most attention is given to the controversial parties that staff members of the British government organised during last year's lockdown. From December 12 onwards, the Formula 1 race ("Hamilton" and "Verstappen") is also covered a lot. In Canada, the topic of cannabis is discussed often throughout all days - as well as important areas such as Toronto, Quebec and Ontario. In Australia, the Ashes (referring to the cricket games between England and Australia) are covered the most after Christmas and Covid throughout all days. In addition, Queensland ("qld") and "Aussies" are also frequently used in headlines. In New-Zealand, apart from "business", most attention is given to the areas of Wellington and Auckland. 

*Figure 3: The development of a selection of words between December 9 (0.01 CET) and 15 (20.00 CET)*

## {.tabset .tabset-fade .tabset-pills}

### All countries {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
## first graph: all newspapers in all countries
graph_data_all<- headlines_tok %>% 
  group_by(time, word) %>% 
  count(word, sort = TRUE, name = "n") 

graph_all <- graph_data_all %>% 
  filter(word == "christmas" | word == "covid" | word == "party" | word == "omicron" | word == "boris" |
         word == "black" | word == "red" | word == "car" | word == "death" | word == "plan" |
         word == "million" | word == "vaccine" | word == "sex" | word == "women" | word == "police" | word == "health" | word == "court" |
         word == "government" | word == "holiday" | word == "fans" | word == "street" | word == "biden")  %>% 
  ggplot(aes(x = time, y = n, group = word, colour = word)) +
  geom_line() +
  labs(title = "The frequency of the 22 most used words in headlines: newspapers of all five countries") +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_discrete(labels = c("2021-12-09-00-01-58"="Dec\n 9",  "2021-12-09-04-01-37"= "", "2021-12-09-08-01-21"= "", "2021-12-09-12-01-27"= "", "2021-12-09-16-01-24" = "", "2021-12-09-20-01-54" = "",
                              "2021-12-10-00-01-51"="Dec\n 10", "2021-12-10-04-01-37"= "", "2021-12-10-08-01-15"= "", "2021-12-10-12-01-29"= "", "2021-12-10-16-02-11" = "", "2021-12-10-20-01-28" = "", 
                              "2021-12-11-00-01-39"="Dec\n 11", "2021-12-11-04-01-51"= "", "2021-12-11-08-01-28"= "", "2021-12-11-12-01-32"= "", "2021-12-11-16-02-28" = "", "2021-12-11-20-01-38" = "",
                              "2021-12-12-00-01-28"="Dec\n 12", "2021-12-12-04-01-25"= "", "2021-12-12-08-01-14"= "", "2021-12-12-12-01-26"= "", "2021-12-12-16-01-27" = "", "2021-12-12-20-01-45" = "",
                              "2021-12-13-00-01-47"="Dec\n 13", "2021-12-13-04-01-19"= "", "2021-12-13-08-01-22"= "", "2021-12-13-12-01-25"= "", "2021-12-13-16-01-32" = "", "2021-12-13-20-01-28" = "",
                              "2021-12-14-00-02-06"="Dec\n 14", "2021-12-14-04-01-58"= "", "2021-12-14-08-01-30"= "", "2021-12-14-12-01-57"= "", "2021-12-14-16-02-10" = "", "2021-12-14-20-01-33" = "",
                              "2021-12-15-00-01-35"="Dec\n 15", "2021-12-15-04-01-28"= "", "2021-12-15-08-01-29"= "", "2021-12-15-12-01-34"= "", "2021-12-15-16-01-52" = "", "2021-12-15-20-01-35" = "")) +
  theme(axis.ticks = element_blank()) +
  theme_minimal()
```

```{r, out.width='100%'}
ggplotly(graph_all) %>% 
  partial_bundle()
```

### US {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
## second graph: all newspapers in the US 
graph_us_data <- headlines_tok %>% 
  group_by(time, word) %>%
  filter(country == "US") %>% 
  count(word, sort = TRUE, name = "n") %>% 
  ungroup()

graph_us <- graph_us_data %>% 
  filter(word == "biden" | word == "omicron" | word == "covid" | word == "court" | word == "holiday" |
         word == "women" | word == "death" | word == "defense" | word == "million" | word == "workers" |
         word == "health" | word == "police" | word == "putin" | word == "school" | word == "booster" | word == "democrats" | word == "florida" | word == "government" | word == "industry" | word == "instagram" | word == "jan" | word == "rep" | word == "shot" | word == "trial" | word == "trump" | word == "vaccine") %>% 
  ggplot(aes(x = time, y = n, group = word, colour = word)) +
  geom_line() +
  geom_point(size = 0.4) +
  labs(title = "The frequency of the most used words in headlines of US newspapers") +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_discrete(labels = c("2021-12-09-00-01-58"="Dec\n 9",  "2021-12-09-04-01-37"= "", "2021-12-09-08-01-21"= "", "2021-12-09-12-01-27"= "", "2021-12-09-16-01-24" = "", "2021-12-09-20-01-54" = "",
                              "2021-12-10-00-01-51"="Dec\n 10", "2021-12-10-04-01-37"= "", "2021-12-10-08-01-15"= "", "2021-12-10-12-01-29"= "", "2021-12-10-16-02-11" = "", "2021-12-10-20-01-28" = "", 
                              "2021-12-11-00-01-39"="Dec\n 11", "2021-12-11-04-01-51"= "", "2021-12-11-08-01-28"= "", "2021-12-11-12-01-32"= "", "2021-12-11-16-02-28" = "", "2021-12-11-20-01-38" = "",
                              "2021-12-12-00-01-28"="Dec\n 12", "2021-12-12-04-01-25"= "", "2021-12-12-08-01-14"= "", "2021-12-12-12-01-26"= "", "2021-12-12-16-01-27" = "", "2021-12-12-20-01-45" = "",
                              "2021-12-13-00-01-47"="Dec\n 13", "2021-12-13-04-01-19"= "", "2021-12-13-08-01-22"= "", "2021-12-13-12-01-25"= "", "2021-12-13-16-01-32" = "", "2021-12-13-20-01-28" = "",
                              "2021-12-14-00-02-06"="Dec\n 14", "2021-12-14-04-01-58"= "", "2021-12-14-08-01-30"= "", "2021-12-14-12-01-57"= "", "2021-12-14-16-02-10" = "", "2021-12-14-20-01-33" = "",
                              "2021-12-15-00-01-35"="Dec\n 15", "2021-12-15-04-01-28"= "", "2021-12-15-08-01-29"= "", "2021-12-15-12-01-34"= "", "2021-12-15-16-01-52" = "", "2021-12-15-20-01-35" = "")) +
  theme(axis.ticks = element_blank()) +
  theme_minimal() 
```

```{r, out.width='100%'}
ggplotly(graph_us) %>% 
  partial_bundle()
```

### UK {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
## third graph: all newspapers in the UK
                       
graph_uk_data <- headlines_tok %>% 
  group_by(time, word) %>%
  filter(country == "UK") %>% 
  count(word, sort = TRUE, name = "n") %>% 
  ungroup()

graph_uk <- graph_uk_data %>% 
    filter(word == "christmas" | word == "covid" | word == "party"  | word == "johnson" |
           word == "omicron" | word == "black" | word == "sex" | word == "celebrity" |
           word == "fans" | word == "husband" | word == "son" | word == "wife" | word == "woman" |
           word == "daughter" | word == "mother" | word == "baby" | word == "chic" |
           word == "death" | word == "government" | word == "vaccine" | word == "police" |
           word == "car" | word == "princess" | word == "vaccine" | word == "tory" | 
           word == "trial" | word == "kardashian" | word == "london") %>% 
  ggplot(aes(x = time, y = n, group = word, colour = word)) +
  geom_line() +
  geom_point(size = 0.4) +
  labs(title = "The frequency of the most used words in headlines of UK newspapers") +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_discrete(labels = c("2021-12-09-00-01-58"="Dec\n 9",  "2021-12-09-04-01-37"= "", "2021-12-09-08-01-21"= "", "2021-12-09-12-01-27"= "", "2021-12-09-16-01-24" = "", "2021-12-09-20-01-54" = "",
                              "2021-12-10-00-01-51"="Dec\n 10", "2021-12-10-04-01-37"= "", "2021-12-10-08-01-15"= "", "2021-12-10-12-01-29"= "", "2021-12-10-16-02-11" = "", "2021-12-10-20-01-28" = "", 
                              "2021-12-11-00-01-39"="Dec\n 11", "2021-12-11-04-01-51"= "", "2021-12-11-08-01-28"= "", "2021-12-11-12-01-32"= "", "2021-12-11-16-02-28" = "", "2021-12-11-20-01-38" = "",
                              "2021-12-12-00-01-28"="Dec\n 12", "2021-12-12-04-01-25"= "", "2021-12-12-08-01-14"= "", "2021-12-12-12-01-26"= "", "2021-12-12-16-01-27" = "", "2021-12-12-20-01-45" = "",
                              "2021-12-13-00-01-47"="Dec\n 13", "2021-12-13-04-01-19"= "", "2021-12-13-08-01-22"= "", "2021-12-13-12-01-25"= "", "2021-12-13-16-01-32" = "", "2021-12-13-20-01-28" = "",
                              "2021-12-14-00-02-06"="Dec\n 14", "2021-12-14-04-01-58"= "", "2021-12-14-08-01-30"= "", "2021-12-14-12-01-57"= "", "2021-12-14-16-02-10" = "", "2021-12-14-20-01-33" = "",
                              "2021-12-15-00-01-35"="Dec\n 15", "2021-12-15-04-01-28"= "", "2021-12-15-08-01-29"= "", "2021-12-15-12-01-34"= "", "2021-12-15-16-01-52" = "", "2021-12-15-20-01-35" = "")) +
  theme(axis.ticks = element_blank()) +
  theme_minimal()

   
```

```{r, out.width='100%'}
ggplotly(graph_uk) %>% 
  partial_bundle()
```

### Canada {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
graph_canada_data <- headlines_tok %>% 
  group_by(time, word) %>%
  filter(country == "Canada") %>% 
  count(word, sort = TRUE, name = "n") %>% 
  ungroup()

graph_canada <- graph_canada_data %>% 
  filter(word == "covid" | word == "canadian" | word == "million" | word == "car" | word == "ontario" | word == "boycott" |
           word == "beijing" | word == "diplomatic" | word == "omicron" | word == "drug" | word == "fight" | word == "hit" |
           word == "investors" | word == "olympics" | word == "winter" | word == "christmas" | word == "fire" |
           word == "health" | word == "holiday" | word == "money" | word == "quebec" | word == "toronto" | word == "alzheimer's" |
           word == "bill" | word == "data" | word == "documents" | word == "federal" | word == "politce" | word == "variant")  %>% 
  ggplot(aes(x = time, y = n, group = word, colour = word)) +
  geom_line() +
  geom_point(size = 0.4) +
  labs(title = "The frequency of the most used words in headlines of Canadian newspapers") +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_discrete(labels = c("2021-12-09-00-01-58"="Dec\n 9",  "2021-12-09-04-01-37"= "", "2021-12-09-08-01-21"= "", "2021-12-09-12-01-27"= "", "2021-12-09-16-01-24" = "", "2021-12-09-20-01-54" = "",
                              "2021-12-10-00-01-51"="Dec\n 10", "2021-12-10-04-01-37"= "", "2021-12-10-08-01-15"= "", "2021-12-10-12-01-29"= "", "2021-12-10-16-02-11" = "", "2021-12-10-20-01-28" = "", 
                              "2021-12-11-00-01-39"="Dec\n 11", "2021-12-11-04-01-51"= "", "2021-12-11-08-01-28"= "", "2021-12-11-12-01-32"= "", "2021-12-11-16-02-28" = "", "2021-12-11-20-01-38" = "",
                              "2021-12-12-00-01-28"="Dec\n 12", "2021-12-12-04-01-25"= "", "2021-12-12-08-01-14"= "", "2021-12-12-12-01-26"= "", "2021-12-12-16-01-27" = "", "2021-12-12-20-01-45" = "",
                              "2021-12-13-00-01-47"="Dec\n 13", "2021-12-13-04-01-19"= "", "2021-12-13-08-01-22"= "", "2021-12-13-12-01-25"= "", "2021-12-13-16-01-32" = "", "2021-12-13-20-01-28" = "",
                              "2021-12-14-00-02-06"="Dec\n 14", "2021-12-14-04-01-58"= "", "2021-12-14-08-01-30"= "", "2021-12-14-12-01-57"= "", "2021-12-14-16-02-10" = "", "2021-12-14-20-01-33" = "",
                              "2021-12-15-00-01-35"="Dec\n 15", "2021-12-15-04-01-28"= "", "2021-12-15-08-01-29"= "", "2021-12-15-12-01-34"= "", "2021-12-15-16-01-52" = "", "2021-12-15-20-01-35" = "")) +
  theme(axis.ticks = element_blank()) +
  theme_minimal()

```

```{r, out.width='100%'}
ggplotly(graph_canada) %>% 
  partial_bundle()
```

### Australia {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
## fifth graph: all newspapers in Australia
graph_australia_data <- headlines_tok %>% 
  group_by(time, word) %>%
  filter(country == "Australia") %>% 
  count(word, sort = TRUE, name = "n") %>% 
  ungroup()

graph_australia <- graph_australia_data %>% 
  filter(word == "covid" | word == "ashes" | word == "aussie" | word == "fight" |
           word == "omicron" | word == "shock" | word == "history" | word == "tests" | word == "syndey" | word == "attack" |
           word == "battle" | word == "deal" | word == "jab" | word == "workers" | word == "court" |
           word == "death" | word == "hits" | word == "positive" | word == "summer" | word == "airport" | 
           word == "barnaby" | word == "cancer" | word == "christmas" | word == "gabba" | word == "health" |
           word == "property" | word == "supercoach" | word == "test" | word == "vaccine") %>% 
  ggplot(aes(x = time, y = n, group = word, colour = word)) +
  geom_line() +
  geom_point(size = 0.4) +
  labs(title = "The frequency of the most used words in headlines of Australian newspapers") +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_discrete(labels = c("2021-12-09-00-01-58"="Dec\n 9",  "2021-12-09-04-01-37"= "", "2021-12-09-08-01-21"= "", "2021-12-09-12-01-27"= "", "2021-12-09-16-01-24" = "", "2021-12-09-20-01-54" = "",
                              "2021-12-10-00-01-51"="Dec\n 10", "2021-12-10-04-01-37"= "", "2021-12-10-08-01-15"= "", "2021-12-10-12-01-29"= "", "2021-12-10-16-02-11" = "", "2021-12-10-20-01-28" = "", 
                              "2021-12-11-00-01-39"="Dec\n 11", "2021-12-11-04-01-51"= "", "2021-12-11-08-01-28"= "", "2021-12-11-12-01-32"= "", "2021-12-11-16-02-28" = "", "2021-12-11-20-01-38" = "",
                              "2021-12-12-00-01-28"="Dec\n 12", "2021-12-12-04-01-25"= "", "2021-12-12-08-01-14"= "", "2021-12-12-12-01-26"= "", "2021-12-12-16-01-27" = "", "2021-12-12-20-01-45" = "",
                              "2021-12-13-00-01-47"="Dec\n 13", "2021-12-13-04-01-19"= "", "2021-12-13-08-01-22"= "", "2021-12-13-12-01-25"= "", "2021-12-13-16-01-32" = "", "2021-12-13-20-01-28" = "",
                              "2021-12-14-00-02-06"="Dec\n 14", "2021-12-14-04-01-58"= "", "2021-12-14-08-01-30"= "", "2021-12-14-12-01-57"= "", "2021-12-14-16-02-10" = "", "2021-12-14-20-01-33" = "",
                              "2021-12-15-00-01-35"="Dec\n 15", "2021-12-15-04-01-28"= "", "2021-12-15-08-01-29"= "", "2021-12-15-12-01-34"= "", "2021-12-15-16-01-52" = "", "2021-12-15-20-01-35" = "")) +
  theme(axis.ticks = element_blank()) +
  theme_minimal()
```

```{r, out.width='100%'}
ggplotly(graph_australia) %>% 
  partial_bundle()
```


### New-Zealand {.tabset .tabset-fade .tabset-pills}

```{r message = FALSE, warning = FALSE, include = FALSE}
graph_nz_data <- headlines_tok %>% 
  group_by(time, word) %>%
  filter(country == "New-Zealand") %>% 
  count(word, sort = TRUE, name = "n") %>% 
  ungroup()

graph_nz <- graph_nz_data %>% 
  filter(word == "christmas" | word == "covid" | word == "auckland" | word == "gifts" | word == "business" | word == "wellington" |
           word == "business" | word == "spy" | word == "vaccine" |
           word == "black" | word == "guide" | word == "plans" |
           word == "bejing" | word == "boss" | word == "canterbury" | word == "coach" | word == "company" |
           word == "december" | word == "kiwi" | word == "light" | word == "luxon" | word == "queenstown" | 
           word == "test" | word == "viva") %>% 
  ggplot(aes(x = time, y = n, group = word, colour = word)) +
  geom_line() +
  geom_point(size = 0.4) +
  labs(title = "The frequency of the most used words in headlines of New-Zealand newspapers") +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_discrete(labels = c("2021-12-09-00-01-58"="Dec\n 9",  "2021-12-09-04-01-37"= "", "2021-12-09-08-01-21"= "", "2021-12-09-12-01-27"= "", "2021-12-09-16-01-24" = "", "2021-12-09-20-01-54" = "",
                              "2021-12-10-00-01-51"="Dec\n 10", "2021-12-10-04-01-37"= "", "2021-12-10-08-01-15"= "", "2021-12-10-12-01-29"= "", "2021-12-10-16-02-11" = "", "2021-12-10-20-01-28" = "", 
                              "2021-12-11-00-01-39"="Dec\n 11", "2021-12-11-04-01-51"= "", "2021-12-11-08-01-28"= "", "2021-12-11-12-01-32"= "", "2021-12-11-16-02-28" = "", "2021-12-11-20-01-38" = "",
                              "2021-12-12-00-01-28"="Dec\n 12", "2021-12-12-04-01-25"= "", "2021-12-12-08-01-14"= "", "2021-12-12-12-01-26"= "", "2021-12-12-16-01-27" = "", "2021-12-12-20-01-45" = "",
                              "2021-12-13-00-01-47"="Dec\n 13", "2021-12-13-04-01-19"= "", "2021-12-13-08-01-22"= "", "2021-12-13-12-01-25"= "", "2021-12-13-16-01-32" = "", "2021-12-13-20-01-28" = "",
                              "2021-12-14-00-02-06"="Dec\n 14", "2021-12-14-04-01-58"= "", "2021-12-14-08-01-30"= "", "2021-12-14-12-01-57"= "", "2021-12-14-16-02-10" = "", "2021-12-14-20-01-33" = "",
                              "2021-12-15-00-01-35"="Dec\n 15", "2021-12-15-04-01-28"= "", "2021-12-15-08-01-29"= "", "2021-12-15-12-01-34"= "", "2021-12-15-16-01-52" = "", "2021-12-15-20-01-35" = "")) +
  theme(axis.ticks = element_blank()) +
  theme_minimal()

```

```{r, out.width='100%'}
ggplotly(graph_nz) %>% 
  partial_bundle()
```




##
Until now, we have visualized and discussed the frequency of certain words between December 9 and 15. However, in order to understand how long certain words remain in the news cycle over time, we focus on a range of words - more specifically, the words that were discussed most often at our starting time point, December 9 at 0.01. This is what *figure 3* shows: the "frequency development" of the words that were most present in the headlines on December 9. It is possible to isolate the development of a single word or multiple words by double clickling these words in the legend. We want to emphasize that the number of words that were most present in headlines vary per country, due to greater or smaller country-level data samples. For the US, we chose to only include the words that were used 5 times or more on December 9 0.01. For the UK (minimum of 13 mentions on December 9 0.01), Canada (minimum of 6 mentions), Australia (minimum of 6 mentions) and New-Zealand (minimum of 5 mentions) we did the same. 

Looking first at the aggregated data of all countries (see tab "all countries"), we can observe that Covid is consistently prevalent in the headlines. In our time period, the use of the word Christmas decreases substantially from December 13 onwards. On December 13 at 12.00 CET, the use of the word omikron increases substantially. In addition, the use of the words "Covid", "Christmas" and "Omikron" like to fluctuate substantially throughout the days. This indicates that new information about these topics is updated and discussed very frequently.However, for most words (e.g. "plan", "car", "police"), we observe the following trend: while they are discussed relatively often on the first time point, their use decreases constantly until our last time point. 

On the country-level, we observe that the UK is the only country that follows the aggregated trend - most likely due to the fact that the headlines of UK newspapers make up a large part of the dataset. In this country, the use of 24 out of 27 words in headlines decreases quickly after December 9. In all other four countries, we observe interesting diverging trends. In the US, 6 out of 25 words (booster, government, Instagram, Putin, Republican ('rep) and shot) lose salience after December 9. In Canada, the use of 13 out of 26 words (Alzheimer's, Beijing, boycott, car, data, diplmatic, documents, drug, fire, hit, investors, money and olympics) decrease substantially after December 9. In Australia, the use 7 out of 28 words in headlines (airport, Barnaby, cancer, health, history, jab and workers)  decreases after December 9: the other 21 words are constantly present in the news headlines. In New-Zealand, 17 out of 23 words lose significance in the course of time. 

#### Conclusions

From December 9 to 15, the speed in which the use of topics in headlines decreases is the highest in the UK, followed by New-Zealand and Canada. In these countries, the topics in the headlines that we identified lose significance relatively quickly - sometimes after just one day, other times after three to four days. The use of words in headlines in the US and Australia appears to be more constant in the researched time period. In these countries, most words remained salient over the time course. In sum, these findings may indicate that newspapers in the UK, New-Zealand and Canada incorporate the developments of news cycles quicker than the US and Australia - countries which discuss the same topics for a longer period of time. However, it is important to reiterate that our short time span of seven days is prevents us from seeing the bigger picture. Our data is insufficient to draw strong conclusions - future research would need to scrape data for a longer period of time to identify patterns.

#### Statement of contribution

We both were equally involved in brainstorming how we want to present the data we scrape in a convincing manner. This involved deciding on which countries to focus on and the visualisations to choose when we present the data. Koen was responsible for the scraping (e.g. tracing xpaths and creating the scraping function) and the visualisations in figures 1 and 3. Abhiram was initially responsible for a shiny dashboard,  which would allow users to filter between countries, days and newspaper type to find out the words most used on a particular day. On further discussions we decided to leave out the dashboard and present the data in a simpler format. (Please find a working dashboard (shiny_dashboard.rmd) in the project directory). In the end, Abhiram created figure 2. Both were involved in the creation and writing of the final report. 